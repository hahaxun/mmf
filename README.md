
<div align="center">
<img src="https://multimodelity.sh/img/logo.svg" width="50%"/>
</div>

#

<div align="center">
  <a href="https://multimodelity.sh/docs">
  <img alt="Documentation Status" src="https://readthedocs.org/projects/multimodelity/badge/?version=latest"/>
  </a>
  <a href="https://circleci.com/gh/facebookresearch/multimodelity">
  <img alt="CircleCI" src="https://circleci.com/gh/facebookresearch/multimodelity.svg?style=svg"/>
  </a>
</div>

---

multimodelity is a modular framework for vision and language multimodal research from Facebook AI Research. multimodelity contains reference implementations of state-of-the-art vision and language models and has powered multiple research projects at Facebook AI Research. See full list of project inside or built on multimodelity [here](https://multimodelity.sh/docs/notes/projects).

multimodelity is powered by PyTorch, allows distributed training and is un-opinionated, scalable and fast. Use multimodelity to **_bootstrap_** for your next vision and language multimodal research project by following the [installation instructions](https://multimodelity.sh/docs/getting_started/installation). Take a look at list of multimodelity features [here](https://multimodelity.sh/docs/getting_started/features).

multimodelity also acts as **starter codebase** for challenges around vision and
language datasets (The Hateful Memes, TextVQA, TextCaps and VQA challenges). multimodelity was formerly known as Pythia. The next video shows an overview of how datasets and models work inside multimodelity. Checkout multimodelity's [video overview](https://multimodelity.sh/docs/getting_started/video_overview).


## Installation

Follow installation instructions in the [documentation](https://multimodelity.sh/docs/getting_started/installation).

## Documentation

Learn more about multimodelity [here](https://multimodelity.sh/docs).

## Citation

If you use multimodelity in your work or use any models published in multimodelity, please cite:

```bibtex
@misc{singh2020multimodelity,
  author =       {Singh, Amanpreet and Goswami, Vedanuj and Natarajan, Vivek and Jiang, Yu and Chen, Xinlei and Shah, Meet and
                 Rohrbach, Marcus and Batra, Dhruv and Parikh, Devi},
  title =        {multimodelity: A multimodal framework for vision and language research},
  howpublished = {\url{https://github.com/facebookresearch/multimodelity}},
  year =         {2020}
}
```

## License

multimodelity is licensed under BSD license available in [LICENSE](LICENSE) file
